![ASL Translator](mediapipe/docs/images/ASL_Translator.png?raw=true "ASL Translator on App")
=======================================================================

[MediaPipe](http://mediapipe.dev) is a framework for building multimodal (eg. video, audio, any time series data) applied ML pipelines. With MediaPipe, a perception pipeline can be built as a graph of modular components, including, for instance, inference models (e.g., TensorFlow, TFLite) and media processing functions.
We are attempting to create an ASL translator using the framework already created. We have so far able to translator ASL letters. We are going to atttempt to interpret words as well in ASL.

![Real-time Hand Tracking](mediapipe/docs/images/mobile/hand_tracking_android_gpu.gif)

> "<em>MediaPipe has made it extremely easy to track the hands of the user. We take the data and interpret it into the ASL letter it is likely to be</em>"

## ML Solutions in MediaPipe

* [Hand Tracking](mediapipe/docs/hand_tracking_mobile_gpu.md)
![hand_tracking](mediapipe/docs/images/mobile/hand_tracking_3d_android_gpu_small.gif)

## Installation
Follow these [instructions](mediapipe/docs/install.md).

## Getting started
See mobile, desktop and Google Coral [examples](mediapipe/docs/examples.md).

## Documentation
We will soon create Documentation to help with the changes we created in this framework.

## Publications
* [On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)

